{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import ErrorMetricsUtils as err\n",
    "from Utils import CorrectnessMetricUtils as cmu\n",
    "from Utils import AuxUtils as auxu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from csv import reader\n",
    "import random\n",
    "\n",
    "\n",
    "class LinearModel():\n",
    "    \n",
    "    def __init__ (self, loss = \"rmse\", model_type = \"linear\", convergence = \"Stochastic Gradiet Descent\", \n",
    "                  penalty = \"None\", include_bias = True, regression_degree = 2, standarsize = True, normalize = True, \n",
    "                  learning_type = \"constant\", max_iter = float(\"inf\"), alpha = 0.001, epsilon = 0.1):\n",
    "        self.alpha = alpha\n",
    "        self.loss = loss\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_type = learning_type\n",
    "        self.theta = None\n",
    "        self.poly = auxu.PolynomialKernel(regression_degree, include_bias)\n",
    "        self.fitted = False\n",
    "        self.model_type = model_type\n",
    "        self.g = transform_fn_g()\n",
    "        self.convergence = convergence\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def call_convergence ():\n",
    "        if (self.convergence == \"Stochatic Gradient Descent\"):\n",
    "            sch_grad_descent (Xk_train, y_train)\n",
    "        elif (self.convergence == \"Normal Equations\"):\n",
    "            normal_eqn (Xk_train, y_train)\n",
    "        elif (self.convergence == \"Newton's Method\"):\n",
    "             newton_method (Xk_train, y_train)\n",
    "        else:\n",
    "            bch_grad_descent (Xk_train, y_train)\n",
    "        return\n",
    "    \n",
    "    # General linear model\n",
    "    def fit (self, train_set):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        X_train = np.asarray(train_set)[:, :-1]\n",
    "        y_train = np.asarray(train_set)[:, -1]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        self.call_convergence()\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, test_set):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        X_test = np.asarray(test_set)[:, :-1]\n",
    "        if (self.standardize):\n",
    "            X_test = auxu.standardize(X_test)\n",
    "        if (self.normalize):\n",
    "            X_test = auxu.normalize(X_test)\n",
    "        Xk_test = self.poly.kernelize(X_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return self.g(Xk_test @ self.theta)\n",
    "        else: \n",
    "            return -1 # some error statement\n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        \"\"\"\n",
    "        ToDO\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "    \n",
    "    def calc_loss (self, y, hypo):\n",
    "        if (self.loss == \"rmse\"):\n",
    "            return err.rmse_calc (y, hypo)\n",
    "        elif (self.loss == \"mae\"):\n",
    "            return err.mae_calc (y, hypo)\n",
    "        elif (self.loss == \"kld\"):\n",
    "            return err.kl_divergence_calc (y, hypo)\n",
    "        elif (self.loss == \"cross\"):\n",
    "            return err.cross_entropy_calc (y, hypo)\n",
    "        else:\n",
    "            return err.mse_calc (y, hypo)\n",
    "    \n",
    "    def neg_gradient (self, X, y, hypo):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Loss Functions not included\n",
    "        What about 0-1 and other functions useful for classifiers?\n",
    "        \n",
    "        \"\"\"\n",
    "        if (self.loss == \"rmse\"):\n",
    "            rmse_err = err.rmse_calc (y, hypo)\n",
    "            return (X.T)@(y-hypo)/rmse_err\n",
    "        elif (self.loss == \"mae\"):\n",
    "            #ToDo\n",
    "            return\n",
    "        elif (self.loss == \"kld\"):\n",
    "            #ToDo\n",
    "            return\n",
    "        elif (self.loss == \"cross\"):\n",
    "            #ToDo\n",
    "            return\n",
    "        else:\n",
    "            return (X.T)@(y-hypo)\n",
    "    \n",
    "    def transform_fn_g (self):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Other generalzations\n",
    "        What needs to cahnge for Locally Weighted Regression?\n",
    "        \"\"\"\n",
    "        if (self.model_type == \"perceptron\"):\n",
    "            return lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        elif (self.model_type == \"logistic\"):\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            return lambda x:x\n",
    "        \n",
    "    def descent_step (neg_gradient):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Optimal Leerning Rate\n",
    "        Should we add different types?\n",
    "        Check different norm orders and how that matters\n",
    "        \"\"\"\n",
    "        if (self.learning_type == \"normalized\"):\n",
    "            beta = self.alpha/np.linalg.norm(neg_gradient)\n",
    "        elif (self.learning_type == \"constant\"):\n",
    "            beta = self.alpha\n",
    "        else:\n",
    "            # optimal learning type\n",
    "            # ToDo\n",
    "            beta = self.alpha\n",
    "        self.theta = self.theta + beta*neg_gradient\n",
    "        return np.linalg.norm(beta*neg_gradient)\n",
    "        \n",
    "    # General Batch Gradient Descent\n",
    "    def bch_gradient_descent (self, X, y):\n",
    "        \"\"\"\n",
    "        Add Regularizer\n",
    "        \"\"\"\n",
    "        g = transform_fn_g()\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = g(X @ self.theta)\n",
    "            neg_gradient = self.neg_gradient(X, y, hypo)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(y, hypo)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    def get_batch (self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        ToDo - more randomization?\n",
    "            1. system time as seed\n",
    "            2. all elements randomized\n",
    "        \"\"\"\n",
    "        random.seed(3)\n",
    "        index = random.randrange(X.shape[0] - batch_size)\n",
    "        return X[index:index+batch_size, :], y[index:index+batch_size]\n",
    "    \n",
    "    # General Stochastic Gradient Descent\n",
    "    def sch_grad_descent (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Add Regularizer\n",
    "        \"\"\"\n",
    "        batch_size = 49\n",
    "        g = transform_fn_g() \n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            X_sgd, y_sgd = self.get_batch (X, y, batch_size)\n",
    "            hypo = g(X_sgd@self.theta)\n",
    "            err = self.calc_loss(y, hypo)\n",
    "            neg_gradient = self.neg_gradient(X_sgd, y_sgd, hypo)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    # Newton's Method to solve regression (assumed MSE loss function)\n",
    "    def newton_method (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Should we make newton_method for other loss functions?\n",
    "        Add Regularizer?\n",
    "        \"\"\"\n",
    "        g = self.transform_fn_g()\n",
    "        iters = 0;\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = self.g (X @ self.theta)\n",
    "            neg_gradient = X.T @ (y - hypo)\n",
    "            hinv = np.linalg.pinv(X.T @ X)\n",
    "            change = hinv @ neg_gradient\n",
    "            if (np.linalg.norm (change, ord = 1) < self.epsilon): \n",
    "                break\n",
    "            self.theta = self.theta + change\n",
    "            iters = iters + 1\n",
    "        return\n",
    "\n",
    "    # Normal Equations to solve regression (assumed MSE loss function)\n",
    "    def normal_eqn (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Should we make normal equations for other loss functions?\n",
    "        \"\"\"\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@((X.T)@y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-fold cross validation with 5 folds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'degrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-601-b18ed2c2d1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mn_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running k-fold cross validation with 5 folds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mmseScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaeScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmseScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceScores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2Scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_algo_cross_val\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSE Loss: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmseScores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MAE Loss: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmseScores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-601-b18ed2c2d1dc>\u001b[0m in \u001b[0;36meval_algo_cross_val\u001b[0;34m(dataset, n_folds, *args)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# how to generalize this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-597-418e3241347b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_set)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mXS_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#XS_test = scaler.fit_transform(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mXk_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXS_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#Xk_test = poly.fit_transform(XS_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# choose convergence method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - IIT Delhi/Courses/ELL409/409 Assignment 1/ELL409-Assignment1-main/Utils/KernelsUtils.py\u001b[0m in \u001b[0;36mkernelize\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdeg\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mfinal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mdeg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mX_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'degrees' is not defined"
     ]
    }
   ],
   "source": [
    "# Get CSV file\n",
    "def get_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        data = reader(file)\n",
    "        for row in data:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#String to float columnwise\n",
    "def str_to_float_col(dataset, col):\n",
    "    for row in dataset:\n",
    "        row[col] = float(row[col].strip())\n",
    "\n",
    "def get_csv2 (filename):\n",
    "    op = np.genfromtxt(filename, delimiter=',')\n",
    "    op = op[:, 1:]\n",
    "    return op.astype(np.float)\n",
    "        \n",
    "# Split dataset into n folds\n",
    "def crossval_split(dataset, n_folds):\n",
    "    split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_dim = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_dim:\n",
    "            index = random.randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        split.append(fold)\n",
    "    return split\n",
    "\n",
    "# Algo evaluation by cross validation split\n",
    "def eval_algo_cross_val (dataset, n_folds, *args):\n",
    "    folds = crossval_split(dataset, n_folds)\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = fold\n",
    "        algo = LinearModel() # how to generalize this\n",
    "        algo.fit(train_set)\n",
    "        predicted = algo.predict(test_set)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        mse = err.mse_calc(actual, predicted)\n",
    "        mae = err.mae_calc(actual, predicted)\n",
    "        rmse = err.rmse_calc(actual, predicted)\n",
    "        #kl_div = err.kl_div_calc(actual, predicted)\n",
    "        #js_div = err.js_div_calc(actual, predicted)\n",
    "        #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "        r2 = err.r2_calc(actual, predicted)\n",
    "        mseScores.append(mse)\n",
    "        maeScores.append(mae)\n",
    "        rmseScores.append(rmse)\n",
    "        #klScores.append(kl_div)\n",
    "        #jsScores.append(js_div)\n",
    "        #ceScores.append(cross_entropy)\n",
    "        r2Scores.append(r2)\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# Train over entire set and report training Error\n",
    "def eval_algo_train (dataset, n_folds, *args):\n",
    "    #new_d = list ()\n",
    "    #new_d.append(dataset[0])\n",
    "    #new_d.append(dataset[1])\n",
    "    #new_d.append(dataset[2])\n",
    "    #new_d.append(dataset[3])\n",
    "    #new_d.append(dataset[4])\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    train_set = list(dataset)\n",
    "    #test_set = list()\n",
    "    #for row in dataset:\n",
    "    #    row_copy = list(row)\n",
    "    #    test_set.append(row_copy)\n",
    "    #    row_copy[-1] = None\n",
    "    algo = LinearModel()\n",
    "    algo.fit(train_set)\n",
    "    predicted = algo.predict(train_set)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    mse = err.mse_calc(actual, predicted)\n",
    "    mae = err.mae_calc(actual, predicted)\n",
    "    rmse = err.rmse_calc(actual, predicted)\n",
    "    #kl_div = err.kl_div_calc(actual, predicted)\n",
    "    #js_div = err.js_div_calc(actual, predicted)\n",
    "    #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "    r2 = err.r2_calc(actual, predicted)\n",
    "    mseScores.append(mse)\n",
    "    maeScores.append(mae)\n",
    "    rmseScores.append(rmse)\n",
    "    #klScores.append(kl_div)\n",
    "    #jsScores.append(js_div)\n",
    "    #ceScores.append(cross_entropy)\n",
    "    r2Scores.append(r2)\n",
    "    #print (len(predicted), \" \", len(actual))\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# evaluate algorithm\n",
    "random.seed(1)\n",
    "filename = 'data/regression.csv'\n",
    "dataset = get_csv(filename)\n",
    "dataset.remove(dataset[0]) # remove headings\n",
    "for i in range(len(dataset[0])): # convert dataset to float columnwise\n",
    "    str_to_float_col(dataset, i) \n",
    "#print (dataset[2])\n",
    "    \n",
    "n_folds = 5\n",
    "print('Running k-fold cross validation with 5 folds')\n",
    "mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores = eval_algo_cross_val (dataset, n_folds)\n",
    "print('MSE Loss: %s' % mseScores)\n",
    "print('MAE Loss: %s' % mseScores)\n",
    "print('RMSE Loss: %s' % rmseScores)\n",
    "print('R squared Loss: %s' % r2Scores)\n",
    "print('KL Divergence: %s' % klScores)\n",
    "print('JS Divergence: %s' % jsScores)\n",
    "print('Cross Entropy: %s' % ceScores)\n",
    "print('Mean MSE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean MAE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean RMSE Loss: %s' % (sum(rmseScores)/float(len(rmseScores))))\n",
    "print('Mean R squared Loss: %s' % (sum(r2Scores)/float(len(r2Scores))))\n",
    "#print('Mean KL Divergence: %s' % (sum(klScores)/float(len(klScores))))\n",
    "#print('Mean JS Divergence: %s' % (sum(jsScores)/float(len(jsScores))))\n",
    "#print('Mean Cross Entropy: %s' % (sum(ceScores)/float(len(ceScores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
