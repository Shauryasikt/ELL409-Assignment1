{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import ErrorMetricsUtils as err\n",
    "from Utils import CorrectnessMetricUtils as cmu\n",
    "from Utils import AuxUtils as auxu\n",
    "from csv import reader, writer\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Lasso\n",
    "class l1_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * np.linalg.norm(W)\n",
    "    def grad(self, W):\n",
    "        return self.alpha * np.sign(W)\n",
    "#Ridge\n",
    "class l2_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * 0.5 *  W.T@W\n",
    "    def grad(self, W):\n",
    "        return self.alpha * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():  \n",
    "    \n",
    "    def __init__ (self, regression_degree = 5, l1_alpha = 0, l2_alpha = 0, loss = \"mse\", model_type = \"linear\", \n",
    "                  convergence = \"Stochastic Gradiet Descent\", include_bias = True, standardize = True, \n",
    "                  normalize = True, learning_type = \"normalized\", max_iter = 1e+4, learning_rate = 0.1, \n",
    "                  epsilon = 0.01, print_stuff = \"prio\"):\n",
    "        self.l1_alpha = l1_alpha\n",
    "        self.l2_alpha = l2_alpha\n",
    "        self.loss = loss\n",
    "        self.model_type = model_type\n",
    "        self.convergence = convergence\n",
    "        self.standardize = standardize\n",
    "        self.normalize = normalize\n",
    "        self.learning_type = learning_type\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = None\n",
    "        self.fitted = False\n",
    "        self.print_stuff = print_stuff\n",
    "        self.poly = auxu.PolynomialKernel(regression_degree, include_bias)\n",
    "        if (self.model_type == \"perceptron\"):\n",
    "            self.g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        elif (self.model_type == \"logistic\"):\n",
    "            self.g = lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            self.g = lambda x:x\n",
    "    \n",
    "    def call_convergence (self, X, y):\n",
    "        if (self.convergence == \"Stochatic Gradient Descent\"):\n",
    "            self.sch_grad_descent (X, y)\n",
    "        elif (self.convergence == \"Normal Equations\"):\n",
    "            self.normal_eqn (X, y)\n",
    "        elif (self.convergence == \"Newton's Method\"):\n",
    "             self.newton_method (X, y)\n",
    "        else:\n",
    "            self.bch_grad_descent (X, y)\n",
    "        return\n",
    "    \n",
    "    def change_degree (self, degree):\n",
    "        self.poly.degree = degree\n",
    "    \n",
    "    # General linear model\n",
    "    def fit (self, X_train, y_train):\n",
    "        #X_train = np.asarray(X)\n",
    "        #y_train = np.asarray(y)\n",
    "        if (X_train.ndim == 1):\n",
    "            X_train = X_train[None, :]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        self.call_convergence(Xk_train, y_train)\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, X_test):\n",
    "        #X_test = np.asarray(X)\n",
    "        if (X_test.ndim == 1):\n",
    "            X_test = X_test[None, :]\n",
    "        if (self.standardize):\n",
    "            X_test = auxu.standardize(X_test)\n",
    "        if (self.normalize):\n",
    "            X_test = auxu.normalize(X_test)\n",
    "        Xk_test = self.poly.kernelize(X_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return self.g(Xk_test @ self.theta)\n",
    "        else: \n",
    "            # some error statement\n",
    "            return -1 \n",
    "    \n",
    "    def log_predict (self, X_test):\n",
    "        disc = self.predict (X_test)\n",
    "        return np.piecewise(disc, [disc < 0.5, disc >= 0.5], [0, 1]) \n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "    \n",
    "    def calc_loss (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        if (self.loss == \"rmse\"):\n",
    "            return err.rmse_calc (y, hypo)\n",
    "        elif (self.loss == \"mae\"):\n",
    "            return err.mae_calc (y, hypo)\n",
    "        elif (self.loss == \"kld\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.kl_divergence_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        elif (self.loss == \"cross_entropy\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.cross_entropy_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        else:\n",
    "            # default case mse\n",
    "            return err.mse_calc (y, hypo)\n",
    "        \n",
    "    def regularizer_neg_gradient(self):\n",
    "        l1 = l1_regularization (self.l1_alpha)\n",
    "        l2 = l2_regularization (self.l2_alpha)\n",
    "        return - l1.grad(self.theta) - l2.grad(self.theta)\n",
    "    \n",
    "    def neg_gradient (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        neg_gradient = np.zeros(self.theta.shape)\n",
    "        if (self.model_type == \"logistic\"):\n",
    "            # only for binary classification\n",
    "            if (self.loss == \"mse\"):\n",
    "                neg_gradient = (X.T)@((y-hypo)*(hypo)*(1-hypo))\n",
    "            elif (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo)*(hypo)*(1-hypo))\n",
    "            else:\n",
    "                # default case: log loss (same as cross entropy loss or kl divergence)\n",
    "                neg_gradient = (X.T)@(y-hypo)\n",
    "        elif (self.model_type == \"perceptron\"):\n",
    "            # only for binary classification\n",
    "            # 0-1 loss by default\n",
    "            neg_gradient = (X.T)@(y-hypo)\n",
    "        else:\n",
    "            # Default model: linear\n",
    "            if (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo))\n",
    "            if (self.loss == \"rmse\"):\n",
    "                rmse_err = self.calc_loss (X, y)\n",
    "                neg_gradient = ((X.T)@(np.sign(y-hypo)))/rmse_err\n",
    "            else: \n",
    "                # default case: mse loss\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "        return neg_gradient + self.regularizer_neg_gradient()\n",
    "        \n",
    "    def descent_step (self, neg_gradient):\n",
    "        if (self.learning_type == \"normalized\"):\n",
    "            beta = self.learning_rate/np.linalg.norm(neg_gradient)\n",
    "        else:\n",
    "            beta = self.learning_rate/100\n",
    "        self.theta = self.theta + beta*neg_gradient\n",
    "        return beta*neg_gradient\n",
    "        \n",
    "    def print_state (self, iters, error, change_mean):\n",
    "        if (self.print_stuff != \"all\"): return\n",
    "        if (iters % 1000 == 0):\n",
    "            print(\"After \", iters, \" steps, the \", self.loss, \" error is \", error, \n",
    "                  \" and the change in theta was \", change_mean)\n",
    "        return\n",
    "        \n",
    "    # General Batch Gradient Descent\n",
    "    def bch_grad_descent (self, X, y):\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            neg_gradient = self.neg_gradient(X, y)\n",
    "            change = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, np.mean(change))\n",
    "            if (np.linalg.norm(change) < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        if (self.print_stuff != \"none\"):\n",
    "            print (\"Converged after \", iters, \" steps\")\n",
    "        return\n",
    "    \n",
    "    def get_batch (self, X, y, batch_size):\n",
    "        random.seed(time.time())\n",
    "        index = random.randrange(X.shape[0] - batch_size)\n",
    "        return X[index:index+batch_size, :], y[index:index+batch_size]\n",
    "    \n",
    "    # General Stochastic Gradient Descent\n",
    "    def sch_grad_descent (self, X, y):\n",
    "        batch_size = 49\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            X_sgd, y_sgd = self.get_batch (X, y, batch_size)\n",
    "            neg_gradient = self.neg_gradient(X_sgd, y_sgd)\n",
    "            change = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, np.mean(change))\n",
    "            if (np.linalg.norm(change) < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        if (self.print_stuff != \"none\"):\n",
    "            print (\"Converged after \", iters, \" steps\")\n",
    "        return\n",
    "    \n",
    "    # Newton's Method to solve regression \n",
    "    # Assumed MSE loss if linear, log loss if logistic and 0-1 loss if perceptron\n",
    "    def newton_method (self, X, y):\n",
    "        iters = 0;\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = self.g (X @ self.theta)\n",
    "            neg_gradient = X.T @ (y - hypo)\n",
    "            hinv = np.linalg.pinv(X.T @ X)\n",
    "            change = hinv @ neg_gradient\n",
    "            if (np.linalg.norm (change, ord = 1) < self.epsilon): \n",
    "                break\n",
    "            self.theta = self.theta + change\n",
    "            iters = iters + 1\n",
    "        return\n",
    "\n",
    "    # Normal Equations to solve regression (assumed MSE loss function and linear/polynomial regression)\n",
    "    def normal_eqn (self, X, y):\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@((X.T)@y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV file\n",
    "def get_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        data = reader(file)\n",
    "        for row in data:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#String to float columnwise\n",
    "def str_to_float_col(dataset, col):\n",
    "    for row in dataset:\n",
    "        row[col] = float(row[col].strip())\n",
    "\n",
    "# Direct input into numpy array somewhat faulty\n",
    "def get_csv2 (filename):\n",
    "    op = np.genfromtxt(filename, delimiter=',')\n",
    "    op = op[:, 1:]\n",
    "    return op.astype(np.float)\n",
    "        \n",
    "def populate_list (glm, X_train, y_train, X_test, y_test, mseScores, maeScores, rmseScores, r2Scores, degree):\n",
    "    glm.change_degree(degree)\n",
    "    glm.fit(X_train, y_train)\n",
    "    mseScores.append([degree, err.mse_calc(y_train, glm.predict(X_train)), err.mse_calc(y_test, glm.predict(X_test))])\n",
    "    maeScores.append([degree, err.mae_calc(y_train, glm.predict(X_train)), err.mae_calc(y_test, glm.predict(X_test))])\n",
    "    rmseScores.append([degree, err.rmse_calc(y_train, glm.predict(X_train)), err.rmse_calc(y_test, glm.predict(X_test))])\n",
    "    r2Scores.append([degree, err.r2_calc(y_train, glm.predict(X_train)), err.r2_calc(y_test, glm.predict(X_test))])\n",
    "\n",
    "# Evaluate linear regression by cross validation split\n",
    "# Predict temp from dewptc, hum, windspd, pressure, rain and smoke\n",
    "def eval_linear (dataset, l1_coeff = 0, l2_coeff = 0, test_fraction = 0.2, max_degree = 10, loss_fn = \"mse\", \n",
    "                 print_stu = \"none\", model = \"linear\", convergence = \"Stochastic Gradient Descent\", *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    X_train, X_test, y_train, y_test = auxu.train_test_split(X, y, test_fraction, False)\n",
    "    mseScores = []\n",
    "    maeScores = []\n",
    "    rmseScores = []\n",
    "    r2Scores = []\n",
    "    degree = 1\n",
    "    glm = LinearModel(degree, l1_coeff, l2_coeff, loss_fn, print_stuff = print_stu, \n",
    "                      model_type = model, convergence = convergence)\n",
    "    # Increasing Model Complexity to see Changing Training Error and Test Error\n",
    "    while (degree <= max_degree):\n",
    "        populate_list (glm, X_train, y_train, X_test, y_test, mseScores, maeScores, rmseScores, r2Scores, degree)\n",
    "        degree = degree + 1\n",
    "    return mseScores, maeScores, rmseScores, r2Scores\n",
    "\n",
    "def test_convergence (dataset, degree = 5, l1_coeff = 0, l2_coeff = 0, loss_fn = \"mse\", *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    glm = LinearModel(degree, l1_coeff, l2_coeff, loss_fn, print_stuff = \"all\")\n",
    "    glm.fit(X, y)\n",
    "    glm.predict(X)\n",
    "    print(err.mse_calc(y, glm.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "filename = 'data/regression.csv'\n",
    "dataset = get_csv(filename)\n",
    "# remove headings\n",
    "dataset.remove(dataset[0]) \n",
    "# convert dataset to float columnwise\n",
    "for i in range(len(dataset[0])): \n",
    "    str_to_float_col(dataset, i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Convergence\")\n",
    "test_convergence (dataset, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bias-Variance Tradeoff Testing\")\n",
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset, max_degree = 5, print_stu = \"none\")\n",
    "filename = \"linear_mse_noreg_test.csv\"\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"BVT_Test\"\n",
    "filepath = os.path.join(here, subdir, filename)\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "file = open (filepath, \"w+\", newline = '')\n",
    "with file:\n",
    "    write = writer (file)\n",
    "    write.writerows(mseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(maeScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(rmseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(r2Scores)\n",
    "print('MSE Loss Data: %s' % mseScores)\n",
    "print('MAE Loss Data: %s' % mseScores)\n",
    "print('RMSE Loss Data: %s' % rmseScores)\n",
    "print('R squared Data: %s' % r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Variance Tradeoff for linear regression using gradient descent\n",
    "l = [0, 0.5, 1.0]\n",
    "losses = [\"mse\", \"mae\"]\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Regression\"\n",
    "\n",
    "# create your subdirectory\n",
    "os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for loss in losses:\n",
    "    for l1 in l:\n",
    "        for l2 in l:\n",
    "            mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset, l1, l2, max_degree = 10, loss_fn = loss)\n",
    "            filename = \"linear_\" + loss + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "            filepath = os.path.join(here, subdir, filename)\n",
    "            file = open (filepath, 'w+', newline = '')\n",
    "            with file:\n",
    "                write = writer (file)\n",
    "                write.writerows(mseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(maeScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(rmseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training/Test Error for linear regression using newton's method\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Newton_Error_Regression\"\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset, max_degree = 10, convergence = \"Newton's Method\")\n",
    "filename = \"newton_errors.csv\"\n",
    "filepath = os.path.join(here, subdir, filename)\n",
    "file = open (filepath, 'w+', newline = '')\n",
    "with file:\n",
    "    write = writer (file)\n",
    "    write.writerows(mseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(maeScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(rmseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "filename = 'data/binary_classification.csv'\n",
    "dataset2 = get_csv(filename)\n",
    "# remove headings\n",
    "dataset2.remove(dataset2[0]) \n",
    "# convert dataset to float columnwise\n",
    "for i in range(len(dataset2[0])): \n",
    "    str_to_float_col(dataset2, i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Variance Tradeoff for logistic regression using gradient descent for classification problem\n",
    "#print(\"Bias-Variance Tradeoff Testing Classification\")\n",
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset2, max_degree = 3, print_stu = \"prio\", \n",
    "                                                          model = \"logistic\", loss_fn = \"log\")\n",
    "filename = \"logistic_log_noreg_test.csv\"\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"BVT_Test\"\n",
    "filepath = os.path.join(here, subdir, filename)\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "file = open (filepath, \"w+\", newline = '')\n",
    "with file:\n",
    "    write = writer (file)\n",
    "    write.writerows(mseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(maeScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(rmseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(r2Scores)\n",
    "print('MSE Loss Data: %s' % mseScores)\n",
    "print('MAE Loss Data: %s' % mseScores)\n",
    "print('RMSE Loss Data: %s' % rmseScores)\n",
    "print('R squared Data: %s' % r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-a7ca93df9f5c>:25: RuntimeWarning: overflow encountered in exp\n",
      "  self.g = lambda x: 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Bias Variance Tradeoff for logistic regression using gradient descent for classification problem\n",
    "\n",
    "l = [0, 0.5, 1.0]\n",
    "losses = [\"mse\", \"mae\", \"log\"]\n",
    "\n",
    "# get the current script path\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Classification\"\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for loss in losses:\n",
    "    for l1 in l:\n",
    "        for l2 in l:\n",
    "            mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset2, l1, l2, max_degree = 10, \n",
    "                                                                      model = \"logistic\", loss_fn = loss)\n",
    "            filename = \"logistic_\" + loss + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "            filepath = os.path.join(here, subdir, filename)\n",
    "            file = open (filepath, 'w+', newline = '')\n",
    "            with file:\n",
    "                write = writer (file)\n",
    "                write.writerows(mseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(maeScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(rmseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0, 0.5, 1.0]\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Classification\"\n",
    "\n",
    "#create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for l1 in l:\n",
    "    for l2 in l:\n",
    "        mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset2, l1, l2, max_degree = 10, \n",
    "                                                                  model = \"perceptron\", loss_fn = \"0-1\")\n",
    "        filename = \"perceptron_\" + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "        filepath = os.path.join(here, subdir, filename)\n",
    "        file = open (filepath, 'w+', newline = '')\n",
    "        with file:\n",
    "            write = writer (file)\n",
    "            write.writerows(mseScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(maeScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(rmseScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
