{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import ErrorMetricsUtils as err\n",
    "from Utils import CorrectnessMetricUtils as cmu\n",
    "#from Utils import KernelsUtils as ker\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from csv import reader\n",
    "import random\n",
    "\n",
    "\n",
    "class LinearModel():\n",
    "    \n",
    "    def __init__ (self, loss = \"rmse\", model_type = \"linear\", learning_type = \"constant\", alpha = 0.001, epsilon = 0.1,):\n",
    "        self.alpha = alpha\n",
    "        self.loss = loss\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_type = learning_type\n",
    "        self.theta = np.zeros(1)\n",
    "        self.poly = PolynomialFeatures(1)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    # General linear regression\n",
    "    def fit (self, train_set):\n",
    "        X_train = np.asarray(train_set)[:, :-1]\n",
    "        y_train = np.asarray(train_set)[:, -1]\n",
    "        #X_test = np.array(test_set)[:, :-1]\n",
    "        #Xk_train = np.ones((X_train.shape[0], X_train.shape[1] + 1))\n",
    "        #Xk_test = np.ones((X_test.shape[0], X_test.shape[1] + 1))\n",
    "        XS_train = self.scaler.fit_transform(X_train)\n",
    "        #XS_test = scaler.fit_transform(X_test)\n",
    "        Xk_train = self.poly.fit_transform(XS_train)\n",
    "        #Xk_test = poly.fit_transform(XS_test)\n",
    "        # choose convergence method\n",
    "        self.theta = bch_grad_descent (Xk_train, y_train, self.model_type, self.loss, self.epsilon, self.alpha, self.learning_type) \n",
    "        #print (Xk_test.shape, \" \", Xk_train.shape, \" \", theta.shape)\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, test_set):\n",
    "        X_test = np.asarray(test_set)[:, :-1]\n",
    "        XS_test = self.scaler.fit_transform(X_test)\n",
    "        Xk_test = self.poly.fit_transform(XS_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return Xk_test @ self.theta\n",
    "        else: \n",
    "            return -1 # some error statement\n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        #print (self.fitted, self.theta.shape, Xk_test.shape[1])\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent\n",
    "def bch_grad_descent (Xk_train, y_train, lin_type, loss_fn, epsilon, alpha, learning_type):\n",
    "    theta = np.zeros(Xk_train.shape[1]) # Xk_train is assumed to be a numpy array\n",
    "    if (lin_type == \"perceptron\"):\n",
    "        g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        return _bch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, g)\n",
    "    elif (lin_type == \"logistic\"):\n",
    "        return _bch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, lambda x: 1 / (1 + np.exp(-x)))\n",
    "    else:\n",
    "        return _bch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, lambda x: x)\n",
    "\n",
    "# Standard ot Batch Gradient Descent Helper (Divide by loss function)\n",
    "def _bch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, g):\n",
    "    if (loss_fn == \"rmse\"):\n",
    "        return bch_grad_descent_rmse (Xk_train, y_train, theta, epsilon, alpha, learning_type, g)\n",
    "    if (loss_fn == \"kld\"):\n",
    "        return bch_grad_descent_kld (Xk_train, y_train, theta, epsilon, alpha, learning_type, g)\n",
    "    else:\n",
    "        return bch_grad_descent_mse (Xk_train, y_train, theta, epsilon, alpha, learning_type, g)\n",
    "\n",
    "# Standard or Batch Gradient Descent under the Mean Squared Error loss function \n",
    "def bch_grad_descent_mse (Xk_train, y_train, theta, epsilon, alpha, lt, g):\n",
    "    hypothesis = g(Xk_train@theta) # matrix multiplication of feature array with parameter array\n",
    "    #print (\"Initial Training Labels:\", y_train, \" with size \", y_train.shape)\n",
    "    #print (\"The feature set is: \", Xk_train, \" with size \", Xk_train.shape)\n",
    "    i = 0\n",
    "    while (True): \n",
    "        mse_err = err.mse_calc(y_train, hypothesis)\n",
    "        neg_gradient = (Xk_train.T)@(y_train-hypothesis)\n",
    "        if (i % 1000 == 0):\n",
    "            #print (\"Interation: \", i)\n",
    "            #print (\"The hypothesis after \", i, \" steps is \", Xk_train@theta)\n",
    "            #print (\"The parameters after \", i , \" steps is \", theta)\n",
    "            print (mse_err)\n",
    "            #print (np.mean(y_train-hypothesis))\n",
    "            #print (\"Difference between prediction and labels after \", i, \" steps is \", y_train-hypothesis)\n",
    "            #print (\"The gradient after \", i, \" steps is \", neg_gradient)\n",
    "        if (lt == \"optimal\"):\n",
    "            beta = alpha/np.linalg.norm(neg_gradient)\n",
    "        else: beta = alpha\n",
    "        change = beta*neg_gradient\n",
    "        if (np.linalg.norm(change) < epsilon): break\n",
    "        theta = theta + change # using matrix operations\n",
    "        i = i + 1\n",
    "        hypothesis = g(Xk_train@theta)\n",
    "    #print (mse_err)\n",
    "    print (\"converged after \", i, \" iterations\")\n",
    "    return theta\n",
    "\n",
    "# Standard or Batch Gradient Descent under the Mean Absolute Error loss function\n",
    "def bch_grad_descent_rmse (Xk_train, y_train, theta, epsilon, alpha, lt, g):\n",
    "    print(1)\n",
    "    hypothesis = g(Xk_train @ theta) # matrix multiplication of feature array with parameter array\n",
    "    while (True):   \n",
    "        rmse_err = err.rmse_calc (y_train, hypothesis)\n",
    "        neg_gradient = (Xk_train.T)@(y_train-hypothesis)\n",
    "        print (rmse_err)\n",
    "        if (lt == \"optimal\"):\n",
    "            beta = alpha/np.linalg.norm(neg_gradient)\n",
    "        else: beta = alpha\n",
    "        change = (beta/rmse_err)*neg_gradient\n",
    "        if (np.linalg.norm(change) < epsilon): break\n",
    "        theta = theta + change # essentially MSE equation divided by RMSE\n",
    "        hypothesis = g(Xk_train@theta)\n",
    "    return theta\n",
    "\n",
    "def bch_grad_descent_kld (Xk_train, y_train, theta, epsilon, alpha, lt, g):\n",
    "    hypothesis = g(Xk_train @ theta) # matrix multiplication of feature array with parameter array\n",
    "    # fix problems with zero denominator and negative values\n",
    "    while (True):\n",
    "        neg_gradient = sum ((y[i]/hypothesis[i])*X_theta[i] for i in range (len(y_train)))\n",
    "        if (lt == \"optimal\"):\n",
    "            beta = alpha/np.linalg.norm(neg_gradient)\n",
    "        else: beta = alpha\n",
    "        change = beta*gradient\n",
    "        if (np.linalg.norm(change) < epsilon): break\n",
    "        theta = theta + change\n",
    "        hypothesis = g(Xk_train@theta)\n",
    "    return theta\n",
    "\n",
    "#def descent_step (theta0, neg_gradient, alpha, cur_err):\n",
    "#    beta = alpha/np.mean(neg_gradient**2)\n",
    "#    #if (curr_err < 5): beta = 1e-2*beta\n",
    "#    #if (curr_err < 4): beta = 1e-2*beta\n",
    "#    return theta + beta*neg_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "def stch_grad_descent (Xk_train, y_train, lin_type, loss_fn, epsilon, alpha, learning_type):\n",
    "    theta = np.zeros(Xk_train.shape[1]) # Xk_train is assumed to be a numpy array\n",
    "    batch_size = 49\n",
    "    if (lin_type == \"perceptron\"):\n",
    "        g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        return _stch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, batch_size, g)\n",
    "    elif (lin_type == \"logistic\"):\n",
    "        g = lambda x: 1 / (1 + np.exp(-x))\n",
    "        return _stch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, batch_size, g)\n",
    "    else:\n",
    "        return _stch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, batch_size, lambda x: x)\n",
    "\n",
    "\n",
    "# Stochastic Gradient Descent Helper (Divide by log function)\n",
    "def _stch_grad_descent (Xk_train, y_train, theta, loss_fn, epsilon, alpha, learning_type, batch_size, g):\n",
    "    if (loss_fn == \"rmse\"):\n",
    "        return stch_grad_descent_rmse (Xk_train, y_train, theta, epsilon, alpha, batch_size, learning_type, g)\n",
    "    if (loss_fn == \"kld\"):\n",
    "        return stch_grad_descent_kld (Xk_train, y_train, theta, epsilon, alpha, batch_size, learning_type, g)\n",
    "    else:\n",
    "        return stch_grad_descent_mse (Xk_train, y_train, theta, epsilon, alpha, batch_size, learning_type, g)\n",
    "\n",
    "def get_batch (Xk_train, y_train, batch_size):\n",
    "    random.seed(3) # replace with system time?\n",
    "    index = random.randrange(Xk_train.shape[0] - batch_size)\n",
    "    return Xk_train[index:index+batch_size, :], y_train[index:index+batch_size]\n",
    "\n",
    "# Stochastic Gradient Descent under the Mean Squared Error loss function \n",
    "def stch_grad_descent_mse (Xk_train, y_train, theta, epsilon, alpha, batch_size, lt, g):\n",
    "    Xk_train_sgd, y_train_sgd = get_batch (Xk_train, y_train, batch_size)\n",
    "    hypothesis = g(Xk_train_sgd@theta) # matrix multiplication of feature array with parameter array\n",
    "    i = 0\n",
    "    while (True): \n",
    "        mse_err = err.mse_calc(y_train_sgd, hypothesis)\n",
    "        neg_gradient = (Xk_train_sgd.T)@(y_train_sgd-hypothesis)\n",
    "        if (i % 10000 == 0):\n",
    "            #print (\"Interation: \", i)\n",
    "            print (mse_err)\n",
    "        if (lt == \"optimal\"):\n",
    "            beta = alpha/np.linalg.norm(neg_gradient)\n",
    "        else: beta = alpha\n",
    "        change = beta*neg_gradient\n",
    "        if (np.linalg.norm(change) < epsilon): break\n",
    "        theta = theta + change # using matrix operations\n",
    "        i = i + 1\n",
    "        Xk_train_sgd, y_train_sgd = get_batch (Xk_train, y_train, batch_size)\n",
    "        hypothesis = g(Xk_train_sgd@theta)\n",
    "    print (mse_err)\n",
    "    print (\"converged after \", i, \" iterations\")\n",
    "    return theta\n",
    "\n",
    "# Stochastic Gradient Descent under the Mean Absolute Error loss function\n",
    "def stch_grad_descent_rmse (Xk_train, y_train, theta, epsilon, alpha, batch_size, lt, g):\n",
    "    Xk_train_sgd, y_train_sgd = get_batch (Xk_train, y_train, batch_size) # take out batch_size samples\n",
    "    hypothesis = g(Xk_train_sgd @ theta) # matrix multiplication of feature array with parameter array\n",
    "    while (True):\n",
    "        rmse_err = err.rmse_calc (y_train_sgd, hypothesis)\n",
    "        print (rmse_err)\n",
    "        neg_gradient = (Xk_train_sgd.T)@(y_train_sgd-hypothesis)\n",
    "        if (lt == \"optimal\"):\n",
    "            beta = alpha/np.linalg.norm(neg_gradient)\n",
    "        else: beta = alpha\n",
    "        change = (beta/rmse_err)*(neg_gradient)\n",
    "        if (np.linalg.norm(change) < epsilon): break\n",
    "        theta = theta + change # essentially MSE equation divided by RMSE\n",
    "        Xk_train_sgd, y_train_sgd = get_batch (Xk_train, y_train, batch_size)\n",
    "        hypothesis = g(Xk_train_sgd@theta)\n",
    "    print (mse_err)\n",
    "    print (\"converged after \", i, \" iterations\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method\n",
    "def newton_method (Xk_train, y_train, lin_type, loss_fn, epsilon, alpha, learning_type):\n",
    "    theta = np.zeros(Xk_train.shape[1]) # Xk_train is assumed to be a numpy array\n",
    "    if (lin_type == \"perceptron\"):\n",
    "        g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        return newton_method_mse (Xk_train, y_train, theta, epsilon, alpha, g)\n",
    "    elif (lin_type == \"logistic\"):\n",
    "        return newton_method_mse (Xk_train, y_train, theta, epsilon, alpha, lambda x: 1 / (1 + np.exp(-x)))\n",
    "    else:\n",
    "        return newton_method_mse (Xk_train, y_train, theta, epsilon, alpha, lambda x: x)\n",
    "\n",
    "def newton_method_mse (Xk_train, y_train, theta, epsilon, alpha, g):\n",
    "    i = 0;\n",
    "    while (True):\n",
    "        neg_gradient = Xk_train.T @ (y_train - g (Xk_train @ theta))\n",
    "        hinv = np.linalg.pinv(*(Xk_train.T @ Xk_train))\n",
    "        change = hinv @ neg_gradient\n",
    "        #print (\"{}th iteration\".format(i + 1))\n",
    "        if (np.linalg.norm (change, ord = 1) < epsilon): \n",
    "            #print(\"too small a change to proceed\");\n",
    "            break\n",
    "        theta = theta + change\n",
    "        i += 1\n",
    "    return theta\n",
    "\n",
    "# Normal Equations to solve regression (assumed MSE loss function)\n",
    "def normal_eqn (Xk_train, y_train, loss_fn, epsilon, alpha, learning_type):\n",
    "    return np.linalg.pinv(Xk_train)@y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-fold cross validation with 5 folds\n",
      "1\n",
      "17.216235070421174\n",
      "16.340936416439778\n",
      "15.47283214284329\n",
      "14.612281107867561\n",
      "13.759691492724848\n",
      "12.91553335592913\n",
      "12.080355448122807\n",
      "11.254808026815878\n",
      "10.439674235420231\n",
      "9.635913877392925\n",
      "8.844725378868521\n",
      "8.067634789388212\n",
      "7.306625409652608\n",
      "6.564328817047575\n",
      "5.8443082417357894\n",
      "5.151477106509012\n",
      "4.492699897812412\n",
      "3.8775838384511556\n",
      "3.3192796540185627\n",
      "2.834525033335776\n",
      "2.4410111806424286\n",
      "2.14978410844609\n",
      "1.9554066301132422\n",
      "1.8353699848348801\n",
      "1.7628187620000488\n",
      "1.71795461143656\n",
      "1.6893441966160154\n",
      "1.6707588185400202\n",
      "1.658612714615736\n",
      "1\n",
      "17.508426542667962\n",
      "16.6332974645103\n",
      "15.765210924923673\n",
      "14.904543148513875\n",
      "14.051722730765288\n",
      "13.207243492754971\n",
      "12.371681528310038\n",
      "11.545718089607643\n",
      "10.730170703996254\n",
      "9.926036039027963\n",
      "9.134549734495465\n",
      "8.357270995011202\n",
      "7.596203584411236\n",
      "6.853970405922881\n",
      "6.134066108179415\n",
      "5.441219214620085\n",
      "4.781893535611963\n",
      "4.164919754403832\n",
      "3.6020953026938236\n",
      "3.1081705404977074\n",
      "2.6988663568741846\n",
      "2.3852665753673774\n",
      "2.165882568425668\n",
      "2.023943934317608\n",
      "1.935409605094565\n",
      "1.8797100190426594\n",
      "1.8436219888846643\n",
      "1.8196245032095515\n",
      "1.8034342431776451\n",
      "1\n",
      "17.502142725963584\n",
      "16.629887220014066\n",
      "15.764481116591355\n",
      "14.906276768651324\n",
      "14.05567563630741\n",
      "13.213140477712217\n",
      "12.37921154626392\n",
      "11.554528378864973\n",
      "10.7398594872182\n",
      "9.936143366355353\n",
      "9.14454591836301\n",
      "8.366541969490399\n",
      "7.604032488240818\n",
      "6.859514946894133\n",
      "6.136332357345921\n",
      "5.439035714620342\n",
      "4.773897960548584\n",
      "4.149589644060493\n",
      "3.5778918302622147\n",
      "3.0739101512629166\n",
      "2.6543948004224953\n",
      "2.3321443563360926\n",
      "2.1071888210426923\n",
      "1.9628159436483175\n",
      "1.8737664272818608\n",
      "1.8183114651459504\n",
      "1.7826833101363926\n",
      "1.7591897121910862\n",
      "1.7434925361106035\n",
      "1\n",
      "16.948488133163973\n",
      "16.073231277430477\n",
      "15.20530748720988\n",
      "14.345110180463251\n",
      "13.493088937308112\n",
      "12.649763912824824\n",
      "11.815745178877814\n",
      "10.991759026088966\n",
      "10.178684226716163\n",
      "9.377602746433062\n",
      "8.589871689314137\n",
      "7.817226805490504\n",
      "7.0619332908075325\n",
      "6.327007491440842\n",
      "5.616543371663687\n",
      "4.936186427054443\n",
      "4.29378829709601\n",
      "3.7001917146051833\n",
      "3.1697899002995658\n",
      "2.7197325717008822\n",
      "2.365607569098344\n",
      "2.1125979630751357\n",
      "1.9482511037918544\n",
      "1.8474205307833451\n",
      "1.7857083210580285\n",
      "1.7468331080981532\n",
      "1.7216468765127044\n",
      "1.7050830740073968\n",
      "1\n",
      "17.303612339624348\n",
      "16.42756187096955\n",
      "15.558673679892665\n",
      "14.697325452451134\n",
      "13.843947823666804\n",
      "12.999037676265964\n",
      "12.163175882061655\n",
      "11.337051272016803\n",
      "10.52149345195272\n",
      "9.717518343889338\n",
      "8.92639226546378\n",
      "8.149723318286124\n",
      "7.389593334754039\n",
      "6.648750174327932\n",
      "5.930888841878187\n",
      "5.241058402345041\n",
      "4.586229053556611\n",
      "3.9760029597892466\n",
      "3.423251875597208\n",
      "2.9439167000854027\n",
      "2.554261330219661\n",
      "2.2638733095481065\n",
      "2.0672310541909464\n",
      "1.9434453937745912\n",
      "1.8672664661199043\n",
      "1.8194027964970172\n",
      "1.7883248998745152\n",
      "1.7676668953508825\n",
      "1.7537863994333482\n",
      "MSE Loss: [4.123628329430382, 3.018929955533383, 4.085397289331393, 6.537080888588907, 2.8190706665537464]\n",
      "MAE Loss: [4.123628329430382, 3.018929955533383, 4.085397289331393, 6.537080888588907, 2.8190706665537464]\n",
      "RMSE Loss: [2.0306718911312043, 1.7375068217228338, 2.0212365743107346, 2.5567715753639213, 1.6790088345669139]\n",
      "R squared Loss: [0.9485903032194998, 0.9517272236148198, 0.944532411416368, 0.920173975247902, 0.961916994685146]\n",
      "KL Divergence: []\n",
      "JS Divergence: []\n",
      "Cross Entropy: []\n",
      "Mean MSE Loss: 4.116821425887562\n",
      "Mean MAE Loss: 4.116821425887562\n",
      "Mean RMSE Loss: 2.0050391394191216\n",
      "Mean R squared Loss: 0.9453881816367472\n"
     ]
    }
   ],
   "source": [
    "# Get CSV file\n",
    "def get_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        data = reader(file)\n",
    "        for row in data:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#String to float columnwise\n",
    "def str_to_float_col(dataset, col):\n",
    "    for row in dataset:\n",
    "        row[col] = float(row[col].strip())\n",
    "\n",
    "def get_csv2 (filename):\n",
    "    op = np.genfromtxt(filename, delimiter=',')\n",
    "    op = op[:, 1:]\n",
    "    return op.astype(np.float)\n",
    "        \n",
    "# Split dataset into n folds\n",
    "def crossval_split(dataset, n_folds):\n",
    "    split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_dim = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_dim:\n",
    "            index = random.randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        split.append(fold)\n",
    "    return split\n",
    "\n",
    "# Algo evaluation by cross validation split\n",
    "def eval_algo_cross_val (dataset, n_folds, *args):\n",
    "    folds = crossval_split(dataset, n_folds)\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = fold\n",
    "        algo = LinearModel() # how to generalize this\n",
    "        algo.fit(train_set)\n",
    "        predicted = algo.predict(test_set)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        mse = err.mse_calc(actual, predicted)\n",
    "        mae = err.mae_calc(actual, predicted)\n",
    "        rmse = err.rmse_calc(actual, predicted)\n",
    "        #kl_div = err.kl_div_calc(actual, predicted)\n",
    "        #js_div = err.js_div_calc(actual, predicted)\n",
    "        #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "        r2 = err.r2_calc(actual, predicted)\n",
    "        mseScores.append(mse)\n",
    "        maeScores.append(mae)\n",
    "        rmseScores.append(rmse)\n",
    "        #klScores.append(kl_div)\n",
    "        #jsScores.append(js_div)\n",
    "        #ceScores.append(cross_entropy)\n",
    "        r2Scores.append(r2)\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# Train over entire set and report training Error\n",
    "def eval_algo_train (dataset, n_folds, *args):\n",
    "    #new_d = list ()\n",
    "    #new_d.append(dataset[0])\n",
    "    #new_d.append(dataset[1])\n",
    "    #new_d.append(dataset[2])\n",
    "    #new_d.append(dataset[3])\n",
    "    #new_d.append(dataset[4])\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    train_set = list(dataset)\n",
    "    #test_set = list()\n",
    "    #for row in dataset:\n",
    "    #    row_copy = list(row)\n",
    "    #    test_set.append(row_copy)\n",
    "    #    row_copy[-1] = None\n",
    "    algo = LinearModel()\n",
    "    algo.fit(train_set)\n",
    "    predicted = algo.predict(train_set)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    mse = err.mse_calc(actual, predicted)\n",
    "    mae = err.mae_calc(actual, predicted)\n",
    "    rmse = err.rmse_calc(actual, predicted)\n",
    "    #kl_div = err.kl_div_calc(actual, predicted)\n",
    "    #js_div = err.js_div_calc(actual, predicted)\n",
    "    #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "    r2 = err.r2_calc(actual, predicted)\n",
    "    mseScores.append(mse)\n",
    "    maeScores.append(mae)\n",
    "    rmseScores.append(rmse)\n",
    "    #klScores.append(kl_div)\n",
    "    #jsScores.append(js_div)\n",
    "    #ceScores.append(cross_entropy)\n",
    "    r2Scores.append(r2)\n",
    "    #print (len(predicted), \" \", len(actual))\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# evaluate algorithm\n",
    "random.seed(1)\n",
    "filename = 'data/regression.csv'\n",
    "dataset = get_csv(filename)\n",
    "dataset.remove(dataset[0]) # remove headings\n",
    "for i in range(len(dataset[0])): # convert dataset to float columnwise\n",
    "    str_to_float_col(dataset, i) \n",
    "#print (dataset[2])\n",
    "    \n",
    "n_folds = 5\n",
    "print('Running k-fold cross validation with 5 folds')\n",
    "mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores = eval_algo_cross_val (dataset, n_folds)\n",
    "print('MSE Loss: %s' % mseScores)\n",
    "print('MAE Loss: %s' % mseScores)\n",
    "print('RMSE Loss: %s' % rmseScores)\n",
    "print('R squared Loss: %s' % r2Scores)\n",
    "print('KL Divergence: %s' % klScores)\n",
    "print('JS Divergence: %s' % jsScores)\n",
    "print('Cross Entropy: %s' % ceScores)\n",
    "print('Mean MSE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean MAE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean RMSE Loss: %s' % (sum(rmseScores)/float(len(rmseScores))))\n",
    "print('Mean R squared Loss: %s' % (sum(r2Scores)/float(len(r2Scores))))\n",
    "#print('Mean KL Divergence: %s' % (sum(klScores)/float(len(klScores))))\n",
    "#print('Mean JS Divergence: %s' % (sum(jsScores)/float(len(jsScores))))\n",
    "#print('Mean Cross Entropy: %s' % (sum(ceScores)/float(len(ceScores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
