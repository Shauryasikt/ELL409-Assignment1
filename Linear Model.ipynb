{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import ErrorMetricsUtils as err\n",
    "from Utils import CorrectnessMetricUtils as cmu\n",
    "from Utils import AuxUtils as auxu\n",
    "from csv import reader\n",
    "import random\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "ToDo\n",
    "1. What is order in norm? Are norms used correctly?\n",
    "2. Optimum Learning Rate\n",
    "3. General Loss Function (instead of 0-1)\n",
    "4. Locally Weighted Linear Regression (if time permits)\n",
    "\"\"\"\n",
    "\n",
    "#Lasso\n",
    "class l1_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * np.linalg.norm(W)\n",
    "    def grad(self, W):\n",
    "        return self.alpha * np.sign(W)\n",
    "#Ridge\n",
    "class l2_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * 0.5 *  W.T@W\n",
    "    def grad(self, W):\n",
    "        return self.alpha * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():  \n",
    "    \n",
    "    def __init__ (self, penalty = \"l1\", l1_alpha = 0, l2_alpha = 0, loss = \"mse\", model_type = \"linear\", \n",
    "                  convergence = \"Stochastic Gradiet Descent\", include_bias = True, regression_degree = 3, \n",
    "                  standardize = True, normalize = True, learning_type = \"constant\", max_iter = float(\"inf\"), \n",
    "                  learning_rate = 0.00001, epsilon = 0.0002):\n",
    "        self.penalty = penalty\n",
    "        self.l1_alpha = l1_alpha\n",
    "        self.l2_alpha = l2_alpha\n",
    "        self.loss = loss\n",
    "        self.model_type = model_type\n",
    "        self.convergence = convergence\n",
    "        self.standardize = standardize\n",
    "        self.normalize = normalize\n",
    "        self.learning_type = learning_type\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = None\n",
    "        self.fitted = False\n",
    "        self.poly = auxu.PolynomialKernel(regression_degree, include_bias)\n",
    "        if (self.model_type == \"perceptron\"):\n",
    "            self.g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        elif (self.model_type == \"logistic\"):\n",
    "            self.g = lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            self.g = lambda x:x\n",
    "    \n",
    "    def call_convergence (self, X, y):\n",
    "        if (self.convergence == \"Stochatic Gradient Descent\"):\n",
    "            self.sch_grad_descent (X, y)\n",
    "        elif (self.convergence == \"Normal Equations\"):\n",
    "            self.normal_eqn (X, y)\n",
    "        elif (self.convergence == \"Newton's Method\"):\n",
    "             self.newton_method (X, y)\n",
    "        else:\n",
    "            self.bch_grad_descent (X, y)\n",
    "        return\n",
    "    \n",
    "    # General linear model\n",
    "    def fit (self, train_set):\n",
    "        X_train = np.asarray(train_set)[:, :-1]\n",
    "        y_train = np.asarray(train_set)[:, -1]\n",
    "        if (X_train.ndim == 1):\n",
    "            X_train = X_train[None, :]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        self.call_convergence(Xk_train, y_train)\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, test_set):\n",
    "        X_test = np.asarray(test_set)[:, :-1]\n",
    "        if (X_test.ndim == 1):\n",
    "            X_test = X_test[None, :]\n",
    "        if (self.standardize):\n",
    "            X_test = auxu.standardize(X_test)\n",
    "        if (self.normalize):\n",
    "            X_test = auxu.normalize(X_test)\n",
    "        Xk_test = self.poly.kernelize(X_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return self.g(Xk_test @ self.theta)\n",
    "        else: \n",
    "            return -1 # some error statement\n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "    \n",
    "    def calc_loss (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        if (self.loss == \"rmse\"):\n",
    "            return err.rmse_calc (y, hypo)\n",
    "        elif (self.loss == \"mae\"):\n",
    "            return err.mae_calc (y, hypo)\n",
    "        elif (self.loss == \"kld\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.kl_divergence_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        elif (self.loss == \"cross_entropy\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.cross_entropy_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        else:\n",
    "            # default case mse\n",
    "            return err.mse_calc (y, hypo)\n",
    "        \n",
    "    def regularizer_neg_gradient(self):\n",
    "        r_neg_g = np.zeros(self.theta.shape)\n",
    "        if \"l1\" in self.penalty:\n",
    "            l1 = l1_regularization (self.l1_alpha)\n",
    "            r_neg_g = r_neg_g - l1.grad(self.theta)\n",
    "        if \"l2\" in self.penalty:\n",
    "            l2 = l2_regularization (self.l2_alpha)\n",
    "            r_neg_g = r_neg_g - l2.grad(self.theta)\n",
    "        return r_neg_g\n",
    "    \n",
    "    def neg_gradient (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        General Loss (with confusion matrix)\n",
    "        Does 0-1 loss work with logistic regression?\n",
    "        \"\"\"\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        neg_gradient = np.zeros(self.theta.shape)\n",
    "        if (self.model_type == \"logistic\"):\n",
    "            # only for binary classification\n",
    "            if (self.loss == \"mse\"):\n",
    "                neg_gradient = (X.T)@((y-hypo)*(hypo)*(1-hypo))\n",
    "            elif (self.loss == \"kld\" or self.loss == \"cross_entropy\"):\n",
    "                neg_gradient = (X.T)@(y*(1-hypo))\n",
    "            elif (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo)*(hypo)*(1-hypo))\n",
    "            else:\n",
    "                # default case: log loss\n",
    "                neg_gradient = (X.T)@(y-hypo)\n",
    "        elif (self.model_type == \"perceptron\"):\n",
    "            # only for binary classification\n",
    "            if (self.loss == \"conf_mat\"):\n",
    "                neg_gradient = neg_gradient\n",
    "            else:\n",
    "                # 0-1 loss by default\n",
    "                neg_gradient = (X.T)@(y-hypo)\n",
    "        else:\n",
    "            # Default model: linear\n",
    "            if (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo))\n",
    "            if (self.loss == \"rmse\"):\n",
    "                rmse_err = self.calc_loss (X, y)\n",
    "                neg_gradient = ((X.T)@(np.sign(y-hypo)))/rmse_err\n",
    "            else: \n",
    "                # default case: mse loss\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "        return neg_gradient + self.regularizer_neg_gradient()\n",
    "        \n",
    "    def descent_step (self, neg_gradient):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Optimal Leerning Rate\n",
    "        \"\"\"\n",
    "        if (self.learning_type == \"normalized\"):\n",
    "            beta = self.learning_rate/np.linalg.norm(neg_gradient)\n",
    "        elif (self.learning_type == \"constant\"):\n",
    "            beta = self.learning_rate\n",
    "        else:\n",
    "            # optimal learning type\n",
    "            # ToDo\n",
    "            beta = self.learning_rate\n",
    "        self.theta = self.theta + beta*neg_gradient\n",
    "        return np.linalg.norm(beta*neg_gradient)\n",
    "        \n",
    "    def print_state (self, iters, error, change_norm):\n",
    "        if (iters % 100 == 0):\n",
    "            print(\"After \", iters, \" steps, the \", self.loss, \" error is \", error, \n",
    "                  \" and the change in theta was \", change_norm)\n",
    "        return\n",
    "        \n",
    "    # General Batch Gradient Descent\n",
    "    def bch_grad_descent (self, X, y):\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            neg_gradient = self.neg_gradient(X, y)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, change_norm)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    def get_batch (self, X, y, batch_size):\n",
    "        random.seed(time.time())\n",
    "        index = random.randrange(X.shape[0] - batch_size)\n",
    "        return X[index:index+batch_size, :], y[index:index+batch_size]\n",
    "    \n",
    "    # General Stochastic Gradient Descent\n",
    "    def sch_grad_descent (self, X, y):\n",
    "        batch_size = 49\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            X_sgd, y_sgd = self.get_batch (X, y, batch_size)\n",
    "            neg_gradient = self.neg_gradient(X_sgd, y_sgd)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, change_norm)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    # Newton's Method to solve regression \n",
    "    # Assumed MSE loss if linear, log loss if logistic and 0-1 loss if perceptron\n",
    "    def newton_method (self, X, y):\n",
    "        iters = 0;\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = self.g (X @ self.theta)\n",
    "            neg_gradient = X.T @ (y - hypo)\n",
    "            hinv = np.linalg.pinv(X.T @ X)\n",
    "            change = hinv @ neg_gradient\n",
    "            if (np.linalg.norm (change, ord = 1) < self.epsilon): \n",
    "                break\n",
    "            self.theta = self.theta + change\n",
    "            iters = iters + 1\n",
    "        return\n",
    "\n",
    "    # Normal Equations to solve regression (assumed MSE loss function and linear/polynomial regression)\n",
    "    def normal_eqn (self, X, y):\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@((X.T)@y)\n",
    "        return"
   ]
  },
