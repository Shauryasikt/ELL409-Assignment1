{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression and Bias-Variance Tradeoff\n",
    "\n",
    "### Linear Model \n",
    "\n",
    "In this notebook, we create a generalized linear model capable of running linear, logistic and perceptron regressions with polynomial kernels, standardized and normalized features with regularization. The linear regression can minimize Mean Squared Error (MSE), Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The logistic regression can work on Log Loss or Cross Entropy loss, Mean Squared Error (MSE) and Mean Absolute Error (MAE). The perceptron only works with a 0-1 loss function. Four Convergence procedures have also been implemented namely Batch Gradient Descent, Stochastic Gradient Descent, Newton's Method and Normal Equations. Newton's Method assumes mse loss for linear regression and log loss for logistic regression while Normal equations are only valued for mse loss for linear regression. \n",
    "\n",
    "### Linear Regression \n",
    "\n",
    "We then implement linear regression on the given dataset https://www.dropbox.com/s/8tqk3cavdbbe3nb/weather_data.xlsx?dl=0 with a few polynomial kernels and regularization coefficients in this notebook and with a cross-validation split and calculate their average test errors (mse loss, rmse loss, mae loss and r2 parameter) to predict temperature. An nth degree kernel implies that all higher degree features are present upto the nth degree. For eg., if a dataset contains three features (x1, x2 and x3), a second degree kernel will have (1, x1, x2, x3, x1^2, x2^2, x3^2, x1x2, x1x3, x2x3) and so on. The regularization coefficients decide which kind of regression will take place. If both l1_alpha and l2_alpha are zero, we have unregularized regression. If l1_alpha is non-zero and l2_alpha is zero, we have l1 regularization, and the resulting regression is called lasso regression. Similarly, if only l2_alpha is non-zero, we have ridge regression and elastic net when both are non-zero.\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "Now, we have three hyperparameters of this general linear model (degree of polynomial kernel and the regularization coefficients). To find the Bias-Variance tradeoff curves, we vary the polynomial degree for a few combinations of regularization coefficients and save the training and test errors for a 80-20 train-test split in csv files. We assume that training errors would correspond to emperical risk while test errors would correspond to the real risk. We then plot the training errors and test errors as a function of model size (here polynomial kernel degree) for each regularization coefficient combination. The regularization coefficients each take discrete values of 0, 0.5 and 1 giving us a total of 9 combinations for each loss function and type of regression. Since we have 3 loss functions for linear regression, we get a total of 18 csv files each containing training and test errors across 4 metrics discussed above (mse error, mae error, rmse error and r2 squared score). We will attach graphs of the variation of these parameters with the model size for a few regularization coefficients in the report and discuss these further.\n",
    "\n",
    "We also repeat the Bias-Variance tradeoff curves for logistic and perceptron regressions applied on the dataset of the first question: https://www.dropbox.com/s/t7ycfw00mc755cg/health_data.csv?dl=0. We have already seen how they perform on the classification tasks with respect to Accuracy, Precision, Specificity and F1 scores, but now we would see to what extent they are successful in reducing training and test errors. We plot the curves for these functions alongside those of linear reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import ErrorMetricsUtils as err\n",
    "import CorrectnessMetricUtils as cmu\n",
    "import AuxUtils as auxu\n",
    "from csv import reader, writer\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "#Lasso\n",
    "class l1_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * np.linalg.norm(W)\n",
    "    def grad(self, W):\n",
    "        return self.alpha * np.sign(W)\n",
    "#Ridge\n",
    "class l2_regularization():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, W):\n",
    "        return self.alpha * 0.5 *  W.T@W\n",
    "    def grad(self, W):\n",
    "        return self.alpha * W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model\n",
    "\n",
    "Inputs the degree of the polynomial kernel, regularization coefficients, loss function (mse, mae, rmse, log, cross entropy, zero-one), model type (linear, perceptron or logistic), convergence type and other parameters to give a robust linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():  \n",
    "    \n",
    "    def __init__ (self, regression_degree = 5, l1_alpha = 0, l2_alpha = 0, loss = \"mse\", model_type = \"linear\", \n",
    "                  convergence = \"Stochastic Gradiet Descent\", include_bias = True, standardize = True, \n",
    "                  normalize = True, learning_type = \"normalized\", max_iter = 1e+4, learning_rate = 0.1, \n",
    "                  epsilon = 0.01, print_stuff = \"prio\"):\n",
    "        self.l1_alpha = l1_alpha\n",
    "        self.l2_alpha = l2_alpha\n",
    "        self.loss = loss\n",
    "        self.model_type = model_type\n",
    "        self.convergence = convergence\n",
    "        self.standardize = standardize\n",
    "        self.normalize = normalize\n",
    "        self.learning_type = learning_type\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = []\n",
    "        self.fitted = False\n",
    "        self.print_stuff = print_stuff\n",
    "        self.poly = auxu.PolynomialKernel(regression_degree, include_bias)\n",
    "        if (self.model_type == \"perceptron\"):\n",
    "            self.g = lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        elif (self.model_type == \"logistic\"):\n",
    "            self.g = lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            self.g = lambda x:x\n",
    "            \n",
    "    def init_theta (self, X_train):\n",
    "        if (X_train.ndim == 1):\n",
    "            X_train = X_train[None, :]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        return\n",
    "    \n",
    "    def repeat_fit (self, X_train, y_train):\n",
    "        if (X_train.ndim == 1):\n",
    "            X_train = X_train[None, :]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        if (not len(self.theta) == Xk_train.shape[1]):\n",
    "            self.init_theta(X_train)\n",
    "        self.call_convergence(Xk_train, y_train)\n",
    "        self.fitted = True\n",
    "        return\n",
    "    \n",
    "    def call_convergence (self, X, y):\n",
    "        if (self.convergence == \"Stochatic Gradient Descent\"):\n",
    "            self.sch_grad_descent (X, y)\n",
    "        elif (self.convergence == \"Normal Equations\"):\n",
    "            self.normal_eqn (X, y)\n",
    "        elif (self.convergence == \"Newton's Method\"):\n",
    "             self.newton_method (X, y)\n",
    "        else:\n",
    "            self.bch_grad_descent (X, y)\n",
    "        return\n",
    "    \n",
    "    def change_degree (self, degree):\n",
    "        self.poly.degree = degree\n",
    "    \n",
    "    # General linear model\n",
    "    def fit (self, X_train, y_train):\n",
    "        #X_train = np.asarray(X)\n",
    "        #y_train = np.asarray(y)\n",
    "        if (X_train.ndim == 1):\n",
    "            X_train = X_train[None, :]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        self.call_convergence(Xk_train, y_train)\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, X_test):\n",
    "        #X_test = np.asarray(X)\n",
    "        if (X_test.ndim == 1):\n",
    "            X_test = X_test[None, :]\n",
    "        if (self.standardize):\n",
    "            X_test = auxu.standardize(X_test)\n",
    "        if (self.normalize):\n",
    "            X_test = auxu.normalize(X_test)\n",
    "        Xk_test = self.poly.kernelize(X_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return self.g(Xk_test @ self.theta)\n",
    "        else: \n",
    "            # some error statement\n",
    "            return -1 \n",
    "    \n",
    "    def log_predict (self, X_test):\n",
    "        disc = self.predict (X_test)\n",
    "        return np.piecewise(disc, [disc < 0.5, disc >= 0.5], [0, 1]) \n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "    \n",
    "    def calc_loss (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        if (self.loss == \"rmse\"):\n",
    "            return err.rmse_calc (y, hypo)\n",
    "        elif (self.loss == \"mae\"):\n",
    "            return err.mae_calc (y, hypo)\n",
    "        elif (self.loss == \"kld\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.kl_divergence_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        elif (self.loss == \"cross_entropy\"):\n",
    "            if (self.model_type == \"logistic\" or self.model_type == \"perceptron\"):\n",
    "                return err.cross_entropy_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"Nan\")\n",
    "        else:\n",
    "            # default case mse\n",
    "            return err.mse_calc (y, hypo)\n",
    "        \n",
    "    def regularizer_neg_gradient(self):\n",
    "        l1 = l1_regularization (self.l1_alpha)\n",
    "        l2 = l2_regularization (self.l2_alpha)\n",
    "        return - l1.grad(self.theta) - l2.grad(self.theta)\n",
    "    \n",
    "    def neg_gradient (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        neg_gradient = np.zeros(self.theta.shape)\n",
    "        if (self.model_type == \"logistic\"):\n",
    "            # only for binary classification\n",
    "            if (self.loss == \"mse\"):\n",
    "                neg_gradient = (X.T)@((y-hypo)*(hypo)*(1-hypo))\n",
    "            elif (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo)*(hypo)*(1-hypo))\n",
    "            else:\n",
    "                # default case: log loss (same as cross entropy loss or kl divergence)\n",
    "                neg_gradient = (X.T)@(y-hypo)\n",
    "        elif (self.model_type == \"perceptron\"):\n",
    "            # only for binary classification\n",
    "            # 0-1 loss by default\n",
    "            neg_gradient = (X.T)@(y-hypo)\n",
    "        else:\n",
    "            # Default model: linear\n",
    "            if (self.loss == \"mae\"):\n",
    "                neg_gradient = (X.T)@(np.sign(y-hypo))\n",
    "            if (self.loss == \"rmse\"):\n",
    "                rmse_err = self.calc_loss (X, y)\n",
    "                neg_gradient = ((X.T)@(np.sign(y-hypo)))/rmse_err\n",
    "            else: \n",
    "                # default case: mse loss\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "        return neg_gradient + self.regularizer_neg_gradient()\n",
    "        \n",
    "    def descent_step (self, neg_gradient):\n",
    "        if (self.learning_type == \"normalized\"):\n",
    "            beta = self.learning_rate/np.linalg.norm(neg_gradient)\n",
    "        else:\n",
    "            beta = self.learning_rate/100\n",
    "        self.theta = self.theta + beta*neg_gradient\n",
    "        return beta*neg_gradient\n",
    "        \n",
    "    def print_state (self, iters, error, change_mean):\n",
    "        if (self.print_stuff != \"all\"): return\n",
    "        if (iters % 1000 == 0):\n",
    "            print(\"After \", iters, \" steps, the \", self.loss, \" error is \", error, \n",
    "                  \" and the change in theta was \", change_mean)\n",
    "        return\n",
    "        \n",
    "    # General Batch Gradient Descent\n",
    "    def bch_grad_descent (self, X, y):\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            neg_gradient = self.neg_gradient(X, y)\n",
    "            change = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, np.mean(change))\n",
    "            if (np.linalg.norm(change) < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        if (self.print_stuff != \"none\"):\n",
    "            print (\"Converged after \", iters, \" steps\")\n",
    "        return\n",
    "    \n",
    "    def get_batch (self, X, y, batch_size):\n",
    "        random.seed(time.time())\n",
    "        index = random.randrange(X.shape[0] - batch_size)\n",
    "        return X[index:index+batch_size, :], y[index:index+batch_size]\n",
    "    \n",
    "    # General Stochastic Gradient Descent\n",
    "    def sch_grad_descent (self, X, y):\n",
    "        batch_size = 49\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            X_sgd, y_sgd = self.get_batch (X, y, batch_size)\n",
    "            neg_gradient = self.neg_gradient(X_sgd, y_sgd)\n",
    "            change = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            self.print_state (iters, err, np.mean(change))\n",
    "            if (np.linalg.norm(change) < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        if (self.print_stuff != \"none\"):\n",
    "            print (\"Converged after \", iters, \" steps\")\n",
    "        return\n",
    "    \n",
    "    # Newton's Method to solve regression \n",
    "    # Assumed MSE loss if linear, log loss if logistic and 0-1 loss if perceptron\n",
    "    def newton_method (self, X, y):\n",
    "        iters = 0;\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = self.g (X @ self.theta)\n",
    "            neg_gradient = X.T @ (y - hypo)\n",
    "            hinv = np.linalg.pinv(X.T @ X)\n",
    "            change = hinv @ neg_gradient\n",
    "            if (np.linalg.norm (change, ord = 1) < self.epsilon): \n",
    "                break\n",
    "            self.theta = self.theta + change\n",
    "            iters = iters + 1\n",
    "        return\n",
    "\n",
    "    # Normal Equations to solve regression (assumed MSE loss function and linear/polynomial regression)\n",
    "    def normal_eqn (self, X, y):\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@((X.T)@y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions to Call Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_list (glm, X_train, y_train, X_test, y_test, mseScores, maeScores, rmseScores, r2Scores, degree):\n",
    "    glm.change_degree(degree)\n",
    "    glm.fit(X_train, y_train)\n",
    "    mseScores.append([degree, err.mse_calc(y_train, glm.predict(X_train)), err.mse_calc(y_test, glm.predict(X_test))])\n",
    "    maeScores.append([degree, err.mae_calc(y_train, glm.predict(X_train)), err.mae_calc(y_test, glm.predict(X_test))])\n",
    "    rmseScores.append([degree, err.rmse_calc(y_train, glm.predict(X_train)), err.rmse_calc(y_test, glm.predict(X_test))])\n",
    "    r2Scores.append([degree, err.r2_calc(y_train, glm.predict(X_train)), err.r2_calc(y_test, glm.predict(X_test))])\n",
    "\n",
    "# Bias-Variance Tradeoff by varying degree of polynomial regression using train-test split\n",
    "# Predict temp from dewptc, hum, windspd, pressure, rain and smoke\n",
    "# Find test and training error over a range of polynomial degrees with fixed l1 and l2\n",
    "def eval_linear (dataset, l1_coeff = 0, l2_coeff = 0, test_fraction = 0.2, max_degree = 10, loss_fn = \"mse\", \n",
    "                 print_stu = \"none\", model = \"linear\", convergence = \"Stochastic Gradient Descent\", \n",
    "                 max_iter = 1e+4, *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    X_train, X_test, y_train, y_test = auxu.train_test_split(X, y, test_fraction, False)\n",
    "    mseScores = []\n",
    "    maeScores = []\n",
    "    rmseScores = []\n",
    "    r2Scores = []\n",
    "    degree = 1\n",
    "    glm = LinearModel(degree, l1_coeff, l2_coeff, loss_fn, print_stuff = print_stu, \n",
    "                      model_type = model, convergence = convergence, max_iter = max_iter)\n",
    "    # Increasing Model Complexity to see Changing Training Error and Test Error\n",
    "    while (degree <= max_degree):\n",
    "        populate_list (glm, X_train, y_train, X_test, y_test, mseScores, maeScores, rmseScores, r2Scores, degree)\n",
    "        degree = degree + 1\n",
    "    return mseScores, maeScores, rmseScores, r2Scores\n",
    "\n",
    "# Bias-Variance Tradeoff with increasing dataset\n",
    "# We will only see this for stochastic gradient descent with mse Error\n",
    "def eval_linear2 (dataset, l1_coeff = 0, l2_coeff = 0, test_fraction = 0.2, curr_degree = 5, \n",
    "                  max_iter = 20, n_epochs = 100, print_stuff = \"none\", *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    X_train, X_test, y_train, y_test = auxu.train_test_split(X, y, test_fraction, False)\n",
    "    mseScores_train = []\n",
    "    mseScores_test = []\n",
    "    glm = LinearModel(curr_degree, l1_coeff, l2_coeff, max_iter = max_iter, print_stuff = print_stuff)\n",
    "    for i in range (n_epochs): \n",
    "        glm.repeat_fit(X_train, y_train)\n",
    "        mseScores_train.append([i, err.mse_calc(y_train, glm.predict(X_train))])\n",
    "        mseScores_test.append ([i, err.mse_calc(y_test, glm.predict(X_test))])\n",
    "    return mseScores_train, mseScores_test\n",
    "\n",
    "# Evaluate Regressions\n",
    "# returns test errors on cross validation\n",
    "def eval_linear3 (dataset, l1_coeff = 0, l2_coeff = 0, n_folds = 5, degree = 5, loss_fn = \"mse\", \n",
    "                 print_stu = \"none\", model = \"linear\", convergence = \"Batch Gradient Descent\", *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    mseScores = []\n",
    "    maeScores = []\n",
    "    rmseScores = []\n",
    "    r2Scores = []\n",
    "    glm = LinearModel(degree, l1_coeff, l2_coeff, loss_fn, print_stuff = print_stu, \n",
    "                      model_type = model, convergence = convergence)\n",
    "    for i in range(n_folds):\n",
    "        X_train, X_test, y_train, y_test = auxu.cross_val_split(X, y, n_folds)[i]\n",
    "        glm.fit (X_train, y_train)\n",
    "        y_test_hat = glm.predict(X_test)\n",
    "        mseScores.append(err.mse_calc(y_test, y_test_hat))\n",
    "        maeScores.append(err.mae_calc(y_test, y_test_hat))\n",
    "        rmseScores.append(err.rmse_calc(y_test, y_test_hat))\n",
    "        r2Scores.append(err.r2_calc(y_test, y_test_hat))\n",
    "    return mseScores, maeScores, rmseScores, r2Scores\n",
    "\n",
    "\n",
    "def test_convergence (dataset, degree = 5, l1_coeff = 0, l2_coeff = 0, loss_fn = \"mse\", *args):\n",
    "    X = np.asarray(dataset)[:, :-1]\n",
    "    y = np.asarray(dataset)[:, -1]\n",
    "    glm = LinearModel(degree, l1_coeff, l2_coeff, loss_fn, print_stuff = \"all\")\n",
    "    glm.fit(X, y)\n",
    "    glm.predict(X)\n",
    "    print(err.mse_calc(y, glm.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV file\n",
    "def get_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        data = reader(file)\n",
    "        for row in data:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#String to float columnwise\n",
    "def str_to_float_col(dataset, col):\n",
    "    for row in dataset:\n",
    "        row[col] = float(row[col].strip())\n",
    "\n",
    "# Direct input into numpy array somewhat faulty\n",
    "def get_csv2 (filename):\n",
    "    op = np.genfromtxt(filename, delimiter=',')\n",
    "    op = op[:, 1:]\n",
    "    return op.astype(np.float)\n",
    "\n",
    "# evaluate algorithm\n",
    "filename = 'weather_data.csv'\n",
    "dataset = get_csv(filename)\n",
    "# remove headings\n",
    "dataset.remove(dataset[0]) \n",
    "# convert dataset to float columnwise\n",
    "for i in range(len(dataset[0])): \n",
    "    str_to_float_col(dataset, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Zone\n",
    "\n",
    "### Unregularized Linear (Polynomial) Regression with fifth degree polynomial kernel (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE Error: 2.085850006197439\n",
      "Mean MAE Error: 0.9104292478330851\n",
      "Mean RMSE Error: 1.394550374152622\n",
      "Mean r2 score: 0.9726267150171536\n"
     ]
    }
   ],
   "source": [
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear3 (dataset, degree = 5)\n",
    "print(\"Mean MSE Error:\", np.mean(mseScores))\n",
    "print(\"Mean MAE Error:\", np.mean(maeScores))\n",
    "print(\"Mean RMSE Error:\", np.mean(rmseScores))\n",
    "print(\"Mean r2 score:\", np.mean(r2Scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Linear (Polynomial) Regression with second degree polynomial kernel (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE Error: 3.9828754077963553\n",
      "Mean MAE Error: 1.3329419880201894\n",
      "Mean RMSE Error: 1.9884327688012007\n",
      "Mean r2 score: 0.9484708027158977\n"
     ]
    }
   ],
   "source": [
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear3 (dataset, l1_coeff = 1.0, l2_coeff = 1.0, degree = 2)\n",
    "print(\"Mean MSE Error:\", np.mean(mseScores))\n",
    "print(\"Mean MAE Error:\", np.mean(maeScores))\n",
    "print(\"Mean RMSE Error:\", np.mean(rmseScores))\n",
    "print(\"Mean r2 score:\", np.mean(r2Scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Convergence\")\n",
    "test_convergence (dataset, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff with increasing data size in linear regression (Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-Variance Tradeoff for increasing dataset size\n",
    "# Linear Regression with stochastic gradient descent epoch-wise\n",
    "mseScores_train, mseScores_test = eval_linear2 (dataset, l1_coeff = 0, l2_coeff = 0, curr_degree = 5, max_iter = 20, n_epochs = 100)\n",
    "#print('Training MSE Score: ', mseScores_train)\n",
    "#print('Testing MSE Score: ', mseScores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1, y1 = np.asarray(mseScores_train).T\n",
    "plt.plot(x1, y1)\n",
    "plt.title('Training Error vs Dataset Size')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Training Error')\n",
    "plt.xlim(-0.1, 100)\n",
    "plt.ylim(-0.1, 100)\n",
    "plt.draw()\n",
    "plt.show()\n",
    "\n",
    "x2, y2 = np.asarray(mseScores_test).T\n",
    "plt.plot(x2, y2)\n",
    "plt.title('Testing Error vs Dataset Size')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Testing Error')\n",
    "plt.xlim(-0.1, 100)\n",
    "plt.ylim(-0.1, 100)\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance tradeoff for linear (polynomial) regression with increasing model size (polynomial degree)\n",
    "\n",
    "#### Using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Variance Tradeoff for linear regression using gradient descent\n",
    "l = [0, 0.5, 1.0]\n",
    "losses = [\"mse\", \"mae\"]\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Regression\"\n",
    "\n",
    "# create your subdirectory\n",
    "os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for loss in losses:\n",
    "    for l1 in l:\n",
    "        for l2 in l:\n",
    "            mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset, l1, l2, max_degree = 10, \n",
    "                                                                      loss_fn = loss, max_iter = 1e+5)\n",
    "            filename = \"linear_\" + loss + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "            filepath = os.path.join(here, subdir, filename)\n",
    "            file = open (filepath, 'w+', newline = '')\n",
    "            with file:\n",
    "                write = writer (file)\n",
    "                write.writerows(mseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(maeScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(rmseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training/Test Error for linear regression using newton's method\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Newton_Error_Regression\"\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset, max_degree = 10, max_iter = 1e+5\n",
    "                                                          convergence = \"Newton's Method\")\n",
    "filename = \"newton_errors.csv\"\n",
    "filepath = os.path.join(here, subdir, filename)\n",
    "file = open (filepath, 'w+', newline = '')\n",
    "with file:\n",
    "    write = writer (file)\n",
    "    write.writerows(mseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(maeScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(rmseScores)\n",
    "    write.writerow([])\n",
    "    write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance tradeoff for classification with increasing model size (polynomial degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate algorithm\n",
    "filename = 'bc.csv'\n",
    "dataset2 = get_csv(filename)\n",
    "# remove headings\n",
    "dataset2.remove(dataset2[0]) \n",
    "# convert dataset to float columnwise\n",
    "for i in range(len(dataset2[0])): \n",
    "    str_to_float_col(dataset2, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Variance Tradeoff for logistic regression using gradient descent for classification problem\n",
    "\n",
    "l = [0, 0.5, 1.0]\n",
    "losses = [\"mse\", \"mae\", \"log\"]\n",
    "\n",
    "# get the current script path\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Classification\"\n",
    "\n",
    "# create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for loss in losses:\n",
    "    for l1 in l:\n",
    "        for l2 in l:\n",
    "            mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset2, l1, l2, max_degree = 10, \n",
    "                                                                      model = \"logistic\", loss_fn = loss)\n",
    "            filename = \"logistic_\" + loss + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "            filepath = os.path.join(here, subdir, filename)\n",
    "            file = open (filepath, 'w+', newline = '')\n",
    "            with file:\n",
    "                write = writer (file)\n",
    "                write.writerows(mseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(maeScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(rmseScores)\n",
    "                write.writerow([])\n",
    "                write.writerows(r2Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Regression (Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0, 0.5, 1.0]\n",
    "\n",
    "# get the current script path.\n",
    "here = os.path.dirname(os.path.realpath('__file__'))\n",
    "subdir = \"Bias_Variance_Tradeoff_Classification\"\n",
    "\n",
    "#create your subdirectory\n",
    "#os.mkdir(os.path.join(here, subdir))\n",
    "\n",
    "for l1 in l:\n",
    "    for l2 in l:\n",
    "        mseScores, maeScores, rmseScores, r2Scores = eval_linear (dataset2, l1, l2, max_degree = 10, \n",
    "                                                                  model = \"perceptron\", loss_fn = \"0-1\")\n",
    "        filename = \"perceptron_\" + \"_l1=\" + str(l1) + \"_l2=\" + str(l2) + \".csv\"\n",
    "        filepath = os.path.join(here, subdir, filename)\n",
    "        file = open (filepath, 'w+', newline = '')\n",
    "        with file:\n",
    "            write = writer (file)\n",
    "            write.writerows(mseScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(maeScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(rmseScores)\n",
    "            write.writerow([])\n",
    "            write.writerows(r2Scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
