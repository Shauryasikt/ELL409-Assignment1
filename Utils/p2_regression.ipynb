{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import ErrorMetricsUtils as err\n",
    "from Utils import CorrectnessMetricUtils as cmu\n",
    "from Utils import AuxUtils as auxu\n",
    "from csv import reader\n",
    "import random\n",
    "\n",
    "# Check different norm orders and how that matters\n",
    "\n",
    "class LinearModel():\n",
    "    \n",
    "    def __init__ (self, penalty = \"None\", l1_alpha = 0, l2_alpha = 0, loss = \"rmse\", model_type = \"linear\", \n",
    "                  convergence = \"Stochastic Gradiet Descent\", include_bias = True, regression_degree = 2, \n",
    "                  standarsize = True, normalize = True, learning_type = \"constant\", max_iter = float(\"inf\"), \n",
    "                  learning_rate = 0.001, epsilon = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_type = learning_type\n",
    "        self.theta = None\n",
    "        self.poly = auxu.PolynomialKernel(regression_degree, include_bias)\n",
    "        self.fitted = False\n",
    "        self.model_type = model_type\n",
    "        self.g = transform_fn_g()\n",
    "        self.convergence = convergence\n",
    "        self.max_iter = max_iter\n",
    "        self.penalty = penalty\n",
    "        self.l1_alpha = l1_alpha\n",
    "        self.l2_alpha = l2_alpha\n",
    "    \n",
    "    def transform_fn_g (self):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Is this the only requirement?\n",
    "        Other generalzations\n",
    "        What needs to change for Locally Weighted Regression?\n",
    "        \"\"\"\n",
    "        if (self.model_type == \"perceptron\"):\n",
    "            return lambda x: np.piecewise(x, [x < 0, x >= 0], [0, 1])\n",
    "        elif (self.model_type == \"logistic\"):\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            return lambda x:x\n",
    "    \n",
    "    def call_convergence ():\n",
    "        if (self.convergence == \"Stochatic Gradient Descent\"):\n",
    "            sch_grad_descent (Xk_train, y_train)\n",
    "        elif (self.convergence == \"Normal Equations\"):\n",
    "            normal_eqn (Xk_train, y_train)\n",
    "        elif (self.convergence == \"Newton's Method\"):\n",
    "             newton_method (Xk_train, y_train)\n",
    "        else:\n",
    "            bch_grad_descent (Xk_train, y_train)\n",
    "        return\n",
    "    \n",
    "    # General linear model\n",
    "    def fit (self, train_set):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        X_train = np.asarray(train_set)[:, :-1]\n",
    "        y_train = np.asarray(train_set)[:, -1]\n",
    "        if (self.standardize):\n",
    "            X_train = auxu.standardize(X_train)\n",
    "        if (self.normalize):\n",
    "            X_train = auxu.normalize(X_train)\n",
    "        Xk_train = self.poly.kernelize(X_train)\n",
    "        self.theta = np.zeros(Xk_train.shape[1])\n",
    "        self.call_convergence()\n",
    "        self.fitted = True\n",
    "        return self.theta\n",
    "    \n",
    "    def predict (self, test_set):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        X_test = np.asarray(test_set)[:, :-1]\n",
    "        if (self.standardize):\n",
    "            X_test = auxu.standardize(X_test)\n",
    "        if (self.normalize):\n",
    "            X_test = auxu.normalize(X_test)\n",
    "        Xk_test = self.poly.kernelize(X_test)\n",
    "        if (self.check_fitted(Xk_test)): \n",
    "            return self.g(Xk_test @ self.theta)\n",
    "        else: \n",
    "            return -1 # some error statement\n",
    "    \n",
    "    def check_fitted (self, Xk_test):\n",
    "        \"\"\"\n",
    "        ToDO\n",
    "        Is this generalizable?\n",
    "        Is it possible for Xk_test to be one dimensional?\n",
    "        Should we heck for X_train being one dimensional and fix?\n",
    "        \"\"\"\n",
    "        return self.fitted and self.theta.shape[0] == Xk_test.shape[1]\n",
    "    \n",
    "    def calc_loss (self, X, y):\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        if (self.loss == \"rmse\"):\n",
    "            return err.rmse_calc (y, hypo)\n",
    "        elif (self.loss == \"mae\"):\n",
    "            return err.mae_calc (y, hypo)\n",
    "        elif (self.loss == \"kld\"):\n",
    "            if (self.model_type == \"logistic\" || self.model_type == \"perceptron\"):\n",
    "                return err.kl_divergence_calc (y, hypo)\n",
    "            else:\n",
    "                return float(\"inf\")\n",
    "        elif (self.loss == \"cross\"):\n",
    "            return err.cross_entropy_calc (y, hypo)\n",
    "        else:\n",
    "            return err.mse_calc (y, hypo)\n",
    "        \n",
    "    def regularizer_neg_gradient(self):\n",
    "        r_neg_g = np.zeros(self.theta.shape)\n",
    "        if \"l1\" in self.penalty:\n",
    "            l1 = auxu.l1_regularization (l1_alpha)\n",
    "            r_neg_g = r_neg_g - l1.grad()\n",
    "        if \"l2\" in self.penalty:\n",
    "            l2 = auxu.l2_regularization (l2_alpha)\n",
    "            r_neg_g = r_neg_g - l2.grad()\n",
    "        return r_neg_g\n",
    "    \n",
    "    def neg_gradient (self, X, y, hypo):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Make Loss Functions not included\n",
    "        What about 0-1 and other functions useful for classifiers?\n",
    "        \n",
    "        \"\"\"\n",
    "        hypo = self.g (X @ self.theta)\n",
    "        neg_gradient = np.zeros(self.theta.shape)\n",
    "        if (model_type == \"linear\"):\n",
    "            if (self.loss == \"mse\"):\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "            elif (self.loss == \"rmse\"):\n",
    "                rmse_err = self.calc_loss (X, y)\n",
    "                neg_gradient = neg_gradient + ((X.T)@(y-hypo))/rmse_err\n",
    "            elif (self.loss == \"mae\"):\n",
    "                #ToDo\n",
    "                neg_gradient = neg_gradient\n",
    "            elif (self.loss == \"cross\"):\n",
    "                #ToDo\n",
    "                neg_gradient = neg_gradient\n",
    "        elif (model_type == \"logistic\"):\n",
    "            if (self.loss == \"log\"):\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "            elif (self.loss == \"mse\"):\n",
    "                neg_gradient = (X.T)@((y-hypo)*hypo*(1-hypo))\n",
    "            elif (self.loss == \"kld\"):\n",
    "                #ToDo\n",
    "                neg_gradient = neg_gradient\n",
    "        elif (model_type == \"perceptron\"):\n",
    "            if (self.loss == \"zero-one\"):\n",
    "                neg_gradient = neg_gradient + (X.T)@(y-hypo)\n",
    "        return neg_gradient + self.regularizer_neg_gradient()\n",
    "        \n",
    "    def descent_step (neg_gradient):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Optimal Leerning Rate\n",
    "        Should we add other different types?\n",
    "        What needs to cahnge for Locally Weighted Regression?\n",
    "        \"\"\"\n",
    "        if (self.learning_type == \"normalized\"):\n",
    "            beta = self.alpha/np.linalg.norm(neg_gradient)\n",
    "        elif (self.learning_type == \"constant\"):\n",
    "            beta = self.alpha\n",
    "        else:\n",
    "            # optimal learning type\n",
    "            # ToDo\n",
    "            beta = self.alpha\n",
    "        self.theta = self.theta + beta*neg_gradient\n",
    "        return np.linalg.norm(beta*neg_gradient)\n",
    "        \n",
    "    # General Batch Gradient Descent\n",
    "    def bch_gradient_descent (self, X, y):\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            neg_gradient = self.neg_gradient(X, y)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    def get_batch (self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        ToDo - more randomization?\n",
    "            1. system time as seed\n",
    "            2. all elements randomized\n",
    "        \"\"\"\n",
    "        random.seed(3)\n",
    "        index = random.randrange(X.shape[0] - batch_size)\n",
    "        return X[index:index+batch_size, :], y[index:index+batch_size]\n",
    "    \n",
    "    # General Stochastic Gradient Descent\n",
    "    def sch_grad_descent (self, X, y):\n",
    "        batch_size = 49\n",
    "        iters = 0\n",
    "        while (iters < self.max_iter):\n",
    "            X_sgd, y_sgd = self.get_batch (X, y, batch_size)\n",
    "            neg_gradient = self.neg_gradient(X_sgd, y_sgd)\n",
    "            change_norm = self.descent_step (neg_gradient)\n",
    "            err = self.calc_loss(X, y)\n",
    "            if (change_norm < self.epsilon): \n",
    "                break\n",
    "            iters = iters + 1\n",
    "        return\n",
    "    \n",
    "    # Newton's Method to solve regression (assumed MSE loss function)\n",
    "    def newton_method (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Should we make newton_method for other loss functions?\n",
    "        Add Regularizer?\n",
    "        \"\"\"\n",
    "        iters = 0;\n",
    "        while (iters < self.max_iter):\n",
    "            hypo = self.g (X @ self.theta)\n",
    "            neg_gradient = X.T @ (y - hypo)\n",
    "            hinv = np.linalg.pinv(X.T @ X)\n",
    "            change = hinv @ neg_gradient\n",
    "            if (np.linalg.norm (change, ord = 1) < self.epsilon): \n",
    "                break\n",
    "            self.theta = self.theta + change\n",
    "            iters = iters + 1\n",
    "        return\n",
    "\n",
    "    # Normal Equations to solve regression (assumed MSE loss function and linear/polynomial regression)\n",
    "    def normal_eqn (self, X, y):\n",
    "        \"\"\"\n",
    "        ToDo\n",
    "        Should we make normal equations for other loss functions?\n",
    "        How will it change for logictov or perceptron\n",
    "        \"\"\"\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@((X.T)@y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV file\n",
    "def get_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        data = reader(file)\n",
    "        for row in data:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#String to float columnwise\n",
    "def str_to_float_col(dataset, col):\n",
    "    for row in dataset:\n",
    "        row[col] = float(row[col].strip())\n",
    "\n",
    "def get_csv2 (filename):\n",
    "    op = np.genfromtxt(filename, delimiter=',')\n",
    "    op = op[:, 1:]\n",
    "    return op.astype(np.float)\n",
    "        \n",
    "# Split dataset into n folds\n",
    "def crossval_split(dataset, n_folds):\n",
    "    split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_dim = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_dim:\n",
    "            index = random.randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        split.append(fold)\n",
    "    return split\n",
    "\n",
    "# Algo evaluation by cross validation split\n",
    "def eval_algo_cross_val (dataset, n_folds, *args):\n",
    "    folds = crossval_split(dataset, n_folds)\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = fold\n",
    "        algo = LinearModel() # how to generalize this\n",
    "        algo.fit(train_set)\n",
    "        predicted = algo.predict(test_set)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        mse = err.mse_calc(actual, predicted)\n",
    "        mae = err.mae_calc(actual, predicted)\n",
    "        rmse = err.rmse_calc(actual, predicted)\n",
    "        #kl_div = err.kl_div_calc(actual, predicted)\n",
    "        #js_div = err.js_div_calc(actual, predicted)\n",
    "        #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "        r2 = err.r2_calc(actual, predicted)\n",
    "        mseScores.append(mse)\n",
    "        maeScores.append(mae)\n",
    "        rmseScores.append(rmse)\n",
    "        #klScores.append(kl_div)\n",
    "        #jsScores.append(js_div)\n",
    "        #ceScores.append(cross_entropy)\n",
    "        r2Scores.append(r2)\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# Train over entire set and report training Error\n",
    "def eval_algo_train (dataset, n_folds, *args):\n",
    "    #new_d = list ()\n",
    "    #new_d.append(dataset[0])\n",
    "    #new_d.append(dataset[1])\n",
    "    #new_d.append(dataset[2])\n",
    "    #new_d.append(dataset[3])\n",
    "    #new_d.append(dataset[4])\n",
    "    mseScores = list()\n",
    "    maeScores = list()\n",
    "    rmseScores = list()\n",
    "    klScores = list()\n",
    "    jsScores = list()\n",
    "    ceScores = list()\n",
    "    r2Scores = list()\n",
    "    train_set = list(dataset)\n",
    "    #test_set = list()\n",
    "    #for row in dataset:\n",
    "    #    row_copy = list(row)\n",
    "    #    test_set.append(row_copy)\n",
    "    #    row_copy[-1] = None\n",
    "    algo = LinearModel()\n",
    "    algo.fit(train_set)\n",
    "    predicted = algo.predict(train_set)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    mse = err.mse_calc(actual, predicted)\n",
    "    mae = err.mae_calc(actual, predicted)\n",
    "    rmse = err.rmse_calc(actual, predicted)\n",
    "    #kl_div = err.kl_div_calc(actual, predicted)\n",
    "    #js_div = err.js_div_calc(actual, predicted)\n",
    "    #cross_entropy = err.cross_entropy_calc(actual, predicted)\n",
    "    r2 = err.r2_calc(actual, predicted)\n",
    "    mseScores.append(mse)\n",
    "    maeScores.append(mae)\n",
    "    rmseScores.append(rmse)\n",
    "    #klScores.append(kl_div)\n",
    "    #jsScores.append(js_div)\n",
    "    #ceScores.append(cross_entropy)\n",
    "    r2Scores.append(r2)\n",
    "    #print (len(predicted), \" \", len(actual))\n",
    "    return mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores\n",
    "\n",
    "# evaluate algorithm\n",
    "random.seed(1)\n",
    "filename = 'data/regression.csv'\n",
    "dataset = get_csv(filename)\n",
    "dataset.remove(dataset[0]) # remove headings\n",
    "for i in range(len(dataset[0])): # convert dataset to float columnwise\n",
    "    str_to_float_col(dataset, i) \n",
    "#print (dataset[2])\n",
    "    \n",
    "n_folds = 5\n",
    "print('Running k-fold cross validation with 5 folds')\n",
    "mseScores, maeScores, rmseScores, klScores, jsScores, ceScores, r2Scores = eval_algo_cross_val (dataset, n_folds)\n",
    "print('MSE Loss: %s' % mseScores)\n",
    "print('MAE Loss: %s' % mseScores)\n",
    "print('RMSE Loss: %s' % rmseScores)\n",
    "print('R squared Loss: %s' % r2Scores)\n",
    "print('KL Divergence: %s' % klScores)\n",
    "print('JS Divergence: %s' % jsScores)\n",
    "print('Cross Entropy: %s' % ceScores)\n",
    "print('Mean MSE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean MAE Loss: %s' % (sum(mseScores)/float(len(mseScores))))\n",
    "print('Mean RMSE Loss: %s' % (sum(rmseScores)/float(len(rmseScores))))\n",
    "print('Mean R squared Loss: %s' % (sum(r2Scores)/float(len(r2Scores))))\n",
    "#print('Mean KL Divergence: %s' % (sum(klScores)/float(len(klScores))))\n",
    "#print('Mean JS Divergence: %s' % (sum(jsScores)/float(len(jsScores))))\n",
    "#print('Mean Cross Entropy: %s' % (sum(ceScores)/float(len(ceScores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
